{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEiGC0xc1l11"
      },
      "source": [
        "# Práctica 1 - Self-Organising Maps - Lab 2\n",
        "## Preparación de entorno\n",
        "#### Instalar las librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HVsKlNq11l12"
      },
      "outputs": [],
      "source": [
        "# %pip install numpy\n",
        "# %pip install matplotlib\n",
        "# %pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEm6Munu1l12"
      },
      "source": [
        "#### Importar librerías de código"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GhVbPRn1l12",
        "outputId": "e8ecdc43-ab43-4afd-c7da-cdc8f56c2900"
      },
      "outputs": [],
      "source": [
        "%reset\n",
        "# from __future__ import division\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import patches as patches\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.lines import Line2D\n",
        "import matplotlib.colors as mcolors\n",
        "from itertools import product\n",
        "\n",
        "# Permite que los gráficos sean interactivos en el notebook\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmFGjXic1l13"
      },
      "source": [
        "## SOM Setup\n",
        "* **SOM** $\\rightarrow$ Red neuronal no supervisada que reduce la dimensionalidad de los datos y los visualiza en un mapa 2D (Mapa autoorganizado).\n",
        "  * Vector de entrada $\\rightarrow$ Se le va presentando al SOM y ajusta sus pesos con estos vectores.\n",
        "  * Autoorganización $\\rightarrow$ Las neuroinas que repsonden de forma siilar a cierta entrada se agrupan.\n",
        "  * Aprendizaje y ajuste $\\rightarrow$ Durante el entrenamiento, la red modifica los donde cada mapa se especializa en un patrón específico.\n",
        "* **Matriz de pesos** $\\rightarrow$ Matriz multidimensionl de neuronas que procesea patrones que vienen de un array de neuronas de entrada.\n",
        "  * Se inicializa con valores aleatorios (no a partir del vector de entrada, si no que se van ajustando a lo largo del entrenamiento).\n",
        "* ***Best Matching Unit (BMU)*** $\\rightarrow$ Neurona que tiene el vector de pesos más similar al vector de entrada.\n",
        "  * El aprendizaje se basa en la actualización de los pesos de la BMU y de las neuronas vecinas.\n",
        "  * Los pesos de la BMU se ajustan/actualizan para acercarse al vector de entrada.\n",
        "* **Vecindad** $\\rightarrow$ Neuronas adyacentes a la BMU (es una región de la matriz de pesos).\n",
        "  * Las neuronas de la vecindad también se ajustan, pero en menor medida que la BMU.\n",
        "* ***Learning Rate (LR)*** $\\rightarrow$ Factor que determina cuánto se ajustan los pesos de la BMU y de las neuronas vecinas.\n",
        "  * Se reduce a lo largo del tiempo para que el ajuste sea más preciso.\n",
        "  * Disminuye a medida que nos alejamos de la BMU.\n",
        "\n",
        "> *Nota* - El SOM aprende a ordenar la matriz de pesos para que reperesente la estructura inherente a los datos de entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "collapsed": true,
        "id": "y6IjU9bS1l13",
        "outputId": "16ed7436-d708-4dbf-947f-b5d56b99fa1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datos cargados: 247 patrones, 18 dimensiones.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Matriz de pesos de las neuronas'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lado_mapa = 100\n",
        "\"\"\"Tamaño del mapa de Kohonen (lado_mapa x lado_mapa)\"\"\"\n",
        "\n",
        "periodo = 1000\n",
        "\"\"\"Número total de iteraciones del entrenamiento\"\"\"\n",
        "\n",
        "learning_rate = 0.5\n",
        "\"\"\"Tasa de aprendizaje inicial (cuanto se modifica el peso en cada iteración)\"\"\"\n",
        "\n",
        "normalizar_datos = True\n",
        "\"\"\"Indica si hay que normalizar los datos o no\"\"\"\n",
        "\n",
        "vecindario = lado_mapa // 2\n",
        "\"\"\"Radio de influenia alrededor de la neurona ganadora\"\"\"\n",
        "\n",
        "dataset_path = \"data/pokemon_train.csv\"\n",
        "datos_original = pd.read_csv(dataset_path)\n",
        "\"\"\"Contiene todos los datos disponibles de cada Pokemon\"\"\"\n",
        "\n",
        "tipos_pokemon = datos_original[['type1', 'type2']].fillna('-').to_numpy()\n",
        "\n",
        "columnas_relevantes = [col for col in datos_original.columns if \"against_\" in col]\n",
        "datos = datos_original[columnas_relevantes].values\n",
        "\"\"\"Filtramos obteniendo únicamente las columnas de cómo reaccionan ante los ataques de los tipos de Pokemon\"\"\"\n",
        "\n",
        "valor_maximo = datos.max(axis=0)\n",
        "\"\"\"Valor máximo de los datos de entrada\"\"\"\n",
        "\n",
        "if normalizar_datos:\n",
        "    # Escalamos los datos al rango [0, 1]\n",
        "    datos = datos / np.max(valor_maximo)\n",
        "\n",
        "num_datos = datos.shape[0]\n",
        "\"\"\"Cantidad de muestras de pokemons que usaremos para entrenar el SOM\"\"\"\n",
        "\n",
        "num_entradas = datos.shape[1]\n",
        "\"\"\"Dimensionalidad de los datos de entrada\"\"\"\n",
        "\n",
        "print(f\"Datos cargados: {num_datos} patrones, {num_entradas} dimensiones.\")\n",
        "\n",
        "matriz_pesos = np.random.random((lado_mapa, lado_mapa, num_entradas))\n",
        "\"\"\"Matriz de pesos de las neuronas\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4srkHFPL1l14"
      },
      "source": [
        "#### Funciones para entrenar/clasificar  \n",
        "##### Distancia euclídea\n",
        "\n",
        "La **distancia euclídea** se emplea para medir la **semejanza** entre el vector de atributos de un Pokémon y los pesos de una neurona. En este laboratorio, cada Pokémon está representado por **18 componentes** \\(`against_*`\\). La fórmula para calcular la distancia euclídea en este caso es:\n",
        "\n",
        "$$\n",
        "\\text{distancia\\_euclídea}(\\mathbf{x}, \\mathbf{w}) \n",
        "= \\sqrt{\\sum_{i=1}^{18} (x_i - w_i)^2}\n",
        "$$\n",
        "\n",
        "donde $\\mathbf{x} = (x_1, x_2, \\dots, x_{18})$ son los valores \\(`against_*`\\) de un Pokémon y $(\\mathbf{w} = (w_1, w_2, \\dots, w_{18}))$ son los pesos de la neurona a comparar. \n",
        "\n",
        "El uso de esta métrica permite localizar la **neurona ganadora (BMU)**, entendida como aquella cuyo vector de pesos minimiza la distancia con respecto al Pokémon de entrada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "id": "a59v6l8b1l14"
      },
      "outputs": [],
      "source": [
        "def calcular_bmu(patron_entrada, matriz_pesos, num_entradas) -> tuple:\n",
        "   \"\"\"Encuentra la BMU (neurona ganadora) para un patrón de entrada.\n",
        "\n",
        "   Args:\n",
        "      patron_entrada: Vector de entrada de un pokemon y sus reacciones ante ataques normalizado\n",
        "      matriz_pesos: Matriz de pesos de las neuronas del SOM\n",
        "      num_entradas: Dimensionalidad de los datos de entrada (18, 1 por cada tipo)\n",
        "\n",
        "   Returns:\n",
        "      tuple (bmu, bmu_idx):\n",
        "         - bmu: vector de pesos de la neurona ganadora\n",
        "         - bmu_idx: coordenadas [x,y] de la neurona ganadora\n",
        "   \"\"\"\n",
        "\n",
        "   distancia_minima = float('inf')\n",
        "   bmu = np.zeros(num_entradas)\n",
        "   bmu_idx = np.zeros(2)\n",
        "\n",
        "   # Recorremos cada neurona para encontrar la BMU\n",
        "   # Filas -> matriz_pesos.shape[0]\n",
        "   # Columnas -> matriz_pesos.shape[1]\n",
        "   for fila in range(matriz_pesos.shape[0]):\n",
        "      for columna in range(matriz_pesos.shape[1]):\n",
        "         peso_actual = matriz_pesos[fila, columna]\n",
        "\n",
        "         # Calculamos la distancia euclídea entre el peso actual y el patrón de entrada\n",
        "         distancia = np.linalg.norm(patron_entrada - peso_actual)\n",
        "\n",
        "         if distancia < distancia_minima:\n",
        "            distancia_minima = distancia\n",
        "            bmu = peso_actual\n",
        "            bmu_idx = np.array([fila, columna])\n",
        "\n",
        "   return bmu, bmu_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu79moUU1l14"
      },
      "source": [
        "##### Learning Rate\n",
        "\n",
        "El learning rate es un valor que se va reduciendo a medida que se va entrenando la red. Controla cuánto se modifican los pesos y determina el temaño del ajuste en los pesos de las neuronas. Se calcula usando una función de decaimiento lineal:\n",
        "$$\\alpha(t) = \\alpha_0 \\cdot \\left ( 1 - \\frac{t}{T} \\right)$$\n",
        "\n",
        "Donde:\n",
        "* $\\alpha_0$: Learning rate inicial.\n",
        "* $t$: Iteración actual.\n",
        "* $T$: Número total de iteraciones.\n",
        "* $\\alpha(t)$: Learning rate en la iteración $t$.\n",
        "\n",
        "¿Porque usamos el decaimiento lineal?:\n",
        "1. **Fase inicial:**\n",
        "   * El learning rate comienza en su valor máximo ($\\alpha_0$).\n",
        "   * La reducción es constante y predecible.\n",
        "   * Los ajustes son proporcionalmente grandes.\n",
        "   * Permite que hagamos modificaciones significativas en el mapa.\n",
        "2. **Fase media:**\n",
        "    * Descenso uniforme del learning rate.\n",
        "    * Los cambios en los pesos son más pequeños.\n",
        "    * Los ajustes son proporcionales al tiempo que ha pasado.\n",
        "    * El mapa comienza a estabilizarse.\n",
        "3. **Fase final:**\n",
        "   * Learning rate se aproxima a 0.\n",
        "   * Los ajustes se vuelven muy pequeños.\n",
        "   * Los cambios son mínimos al final del entrenamiento.\n",
        "   * El mapa tiene a una configuración final estable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": true,
        "id": "45ybXDk21l14"
      },
      "outputs": [],
      "source": [
        "def variacion_learning_rate(learning_rate_inicial, iteracion_actual, num_iteraciones) -> float:\n",
        "   \"\"\"Calcula el Learning Rate (eta) para  la iteración actual utilizando el decaimiento lineal.\n",
        "\n",
        "   Args:\n",
        "      learning_rate_inicial: Learning rate inicial\n",
        "      iteracion_actual: Iteración actual\n",
        "      num_iteraciones: Número total de iteraciones\n",
        "\n",
        "   Returns:\n",
        "      float: Learning rate actualizado para la iteración actual\n",
        "   \"\"\"\n",
        "\n",
        "   return learning_rate_inicial * ( 1 - (iteracion_actual / num_iteraciones))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z24hNggl1l14"
      },
      "source": [
        "##### Vecindario\n",
        "\n",
        "El vencindario es una región de influencia alrededor de la neurona ganadora que determina qué otrsa neuronas se actualizarán y en que medida. Es como una zona de impacto y también se calcula usando la función de decaimiento lineal:\n",
        "$$\\sigma(t) = 1 + \\left ( \\sigma_0 \\cdot \\left (1 - \\frac{t}{T} \\right) \\right)$$\n",
        "\n",
        "Donde:\n",
        "* $\\sigma_0$: Radio inicial del vecindario.\n",
        "* $t$: Iteración actual.\n",
        "* $T$: Número total de iteraciones.\n",
        "* $\\sigma(t)$: Radio del venciandario en la iteración $t$.\n",
        "\n",
        "> *Nota* -> Añadimos el 1 para evitar que el radio del vecindario sea 0 en algún momento.\n",
        "\n",
        "¿Porque usamos el decaimiento lineal?:\n",
        "1. **Fase inicial:**\n",
        "   * El vecindario comienza en su valor máximo ($\\sigma_0$).\n",
        "   * Afecta a un gran número de neuronas vecinas.\n",
        "   * Permite que hagamos una organización global del mapa.\n",
        "   * Facilita que distribuyamos inicialmente las características.\n",
        "2. **Fase media:**\n",
        "    * Reducimos constantemente el radio del vecindario.\n",
        "    * Vamos disminuyendo la influencia gradualmente.\n",
        "    * Formamos estructuras locales más definidas.\n",
        "    * Los ajustes son más precisos y específicos.\n",
        "3. **Fase final:**\n",
        "   * El radio del vecindario se acerca linelamente a 0.\n",
        "   * Los ajustes son en un área muy pequeña.\n",
        "   * Se ajustan los últimos detalles.\n",
        "   * La influencia está practicamente limitada a la BMU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": true,
        "id": "JY93tY2u1l14"
      },
      "outputs": [],
      "source": [
        "def variacion_vecindario(vecindario_inicial, iteracion_actual, num_iteraciones) -> float:\n",
        "   \"\"\"Calcula el radio del vecindario para la iteración actual.\n",
        "\n",
        "   Args:\n",
        "      vecindario_inicial: Radio inicial del vecindario\n",
        "      iteracion_actual: Iteración actual\n",
        "      num_iteraciones: Número total de iteraciones\n",
        "\n",
        "   Returns:\n",
        "      float: Vecindario actualizado para la iteración actual\n",
        "   \"\"\"\n",
        "\n",
        "   return 1 + (vecindario_inicial * ( 1 - (iteracion_actual / num_iteraciones)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qposRnMM1l14"
      },
      "source": [
        "##### Decay (Amortiguación)\n",
        "\n",
        "El decay es una función que determina cómo se amortigua el learning rate según la distancia entre una neurona y la BMU (neurona ganadora). Se calcula usando una función gaussiana:\n",
        "\n",
        "$$h(d,\\sigma) = e^{-\\frac{d^2}{2\\sigma^2}}$$\n",
        "\n",
        "Donde:\n",
        "* $d$: Distancia entre la neurona y la BMU\n",
        "* $\\sigma$: Radio actual del vecindario\n",
        "* $h(d,\\sigma)$: Factor de amortiguación\n",
        "\n",
        "¿Por qué usamos una función gaussiana?:\n",
        "1. **Centro (BMU)**:\n",
        "   * Distancia = 0\n",
        "   * Máxima influencia ($\\approx 1$)\n",
        "   * Mayor ajuste de pesos\n",
        "\n",
        "2. **Neuronas cercanas**:\n",
        "   * Distancia pequeña\n",
        "   * Influencia moderada\n",
        "   * Ajuste proporcional a la cercanía\n",
        "\n",
        "3. **Neuronas lejanas**:\n",
        "   * Distancia grande\n",
        "   * Influencia mínima ($\\approx 0$)\n",
        "   * Casi no se modifican"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "collapsed": true,
        "id": "WXZ2Vvfj1l15"
      },
      "outputs": [],
      "source": [
        "def decay(distancia_BMU, vecindario_actual) -> float:\n",
        "   \"\"\"Calcula la amortiguación de eta en función de la distancia en el mapa entre una neurona y la BMU.\n",
        "\n",
        "   Args:\n",
        "      distancia_BMU: Distancia entre la neurona y la BMU\n",
        "      vecindario_actual: Radio actual del vecindario\n",
        "\n",
        "   Returns:\n",
        "      float: Factor de amortiguación para la iteración (Está entre 0 y 1)\n",
        "   \"\"\"\n",
        "   return np.exp(-distancia_BMU / (2 * (vecindario_actual**2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WSvFn9Y1l15"
      },
      "source": [
        "##### Error de Cuantificación\n",
        "\n",
        "El error de cuantificación mide la diferencia existente entre los datos proporcionados de entrada y los resultados del SOM. Calculamos la distancia promedio entre la BMU y cada patrón:\n",
        "\n",
        "$$EQ = \\frac{1}{N} \\sum_{i=1}^{N} \\| x_i - \\text{BMU}(x_i) \\|$$\n",
        "\n",
        "Donde:\n",
        "* $N$: Número total de patrones de entrada\n",
        "* $x_i$: Patrón de entrada\n",
        "* $\\text{BMU}(x_i)$: Neurona ganadora, más cercana al patrón\n",
        "* $|| x_i - \\text{BMU}(x_i) ||$: Distancia euclídea\n",
        "\n",
        "¿Que utilidad tiene calcular este error?:\n",
        "1. **Error bajo: $$EQ \\approx 0 - 0.1$$**\n",
        "   * Nos muestra una buena representación del dataset\n",
        "   * Sabemos con firmeza que los patrones tienen la BMU correctamente asignada\n",
        "   * Nos demuestra que el SOM es capaz de generalizar con bastante acierto\n",
        "\n",
        "2. **Error alto: $$EQ \\geq 0.3$$**\n",
        "   * Indice sobreentrenamiento (overfitting), o que existen pocos nodos en el mapa.\n",
        "   * La red no presentará de forma adecuada los datos\n",
        "\n",
        "3. **Interpretación práctica**:\n",
        "   * Permite visualizar la calidad del entrenamiento del SOM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "OX9SecAm1l15"
      },
      "outputs": [],
      "source": [
        "def error_cuantificacion(datos, matriz_pesos):\n",
        "    \"\"\"Calcula el error de cuantificación entre los datos y los resultados del SOM.\n",
        "\n",
        "   Args:\n",
        "      datos: Dataset normalizado de los pokemons\n",
        "      matriz_pesos: Matriz de pesos de las neuronas del SOM\n",
        "\n",
        "   Returns:\n",
        "      float: Error de Cuantificación total (mayor o igual que 0)\n",
        "   \"\"\"\n",
        "    error_total = 0\n",
        "    for patron in datos:\n",
        "        _, bmu_idx = calcular_bmu(patron, matriz_pesos, num_entradas)\n",
        "        error_total += np.linalg.norm(patron - matriz_pesos[bmu_idx[0], bmu_idx[1]])\n",
        "    return error_total / len(datos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5gCg1g61l15"
      },
      "source": [
        "##### Error Topológico\n",
        "\n",
        "El error topológico mide las inconsistencias o las violaciones de las relaciones espaciales que definen como se deben de conectar o superponer distintas entidades en un espacio definido por el mapa autoorganizado. Se calcula como la fracción de patrones cuya segunda neurona ganadora BMU-2 no sea vecina de la neurona ganadora BMU-1:\n",
        "\n",
        "$$ET = \\frac{1}{N} \\sum_{i=1}^{N} f(x_i)$$\n",
        "\n",
        "Donde:\n",
        "* $N$: Número total de patrones de entrada\n",
        "* $x_i$: Patrón de entrada\n",
        "* $f(x_i)$: Es igual a 1 si el primer y segundo BMU no están próximos el uno al otro. De otro modo, es 0.\n",
        "\n",
        "¿Que utilidad tiene calcular este error?:\n",
        "1. **Error bajo: $$ET \\approx 0 - 0.1$$**\n",
        "   * Buena preservación de la topología\n",
        "   * Las neuronas de un vecindario presentan resultados similares\n",
        "\n",
        "2. **Error alto: $$ET \\geq 0.3$$**\n",
        "   * Mala preservación de la topología\n",
        "   * Patrones de entrada similares pueden estar en zonas muy dispersas del mapa\n",
        "\n",
        "3. **Interpretación práctica**:\n",
        "   * Permite identificar un número insuficiente de neuronas o una mala configuración general del SOM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "irPm7eoz1l15"
      },
      "outputs": [],
      "source": [
        "def es_vecino(bmu1_idx, bmu2_idx):\n",
        "    x1, y1 = bmu1_idx\n",
        "    x2, y2 = bmu2_idx\n",
        "    return abs(x1 - x2) + abs(y1 - y2) == 1\n",
        "\n",
        "def error_topologico(datos, matriz_pesos):\n",
        "    \"\"\"\n",
        "    Calcula el Error Topológico del SOM.\n",
        "\n",
        "    Args:\n",
        "        datos: Dataset normalizado de los patrones de entrada.\n",
        "        matriz_pesos: Matriz de pesos del SOM.\n",
        "\n",
        "    Returns:\n",
        "        float: Error Topológico (0 <= ET <= 1).\n",
        "    \"\"\"\n",
        "    errores = 0\n",
        "\n",
        "    for patron in datos:\n",
        "        _, bmu1_idx = calcular_bmu(patron, matriz_pesos, num_entradas)\n",
        "\n",
        "        # Buscamos la BMU-2 (segunda mejor neurona)\n",
        "        distancias = np.linalg.norm(matriz_pesos - patron, axis=2)\n",
        "        distancias[bmu1_idx[0], bmu1_idx[1]] = np.inf  # Ignorar BMU-1\n",
        "        bmu2_idx = np.unravel_index(np.argmin(distancias), distancias.shape)\n",
        "\n",
        "        # Si BMU-2 no es vecina de BMU-1, contar como error\n",
        "        if not es_vecino(bmu1_idx, bmu2_idx):\n",
        "            errores += 1\n",
        "\n",
        "    # Una vez sumados todos los errores del mapa, calculamos el promedio de error\n",
        "    return errores / len(datos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgiE2wah1l15"
      },
      "source": [
        "#### Funciones para dibujar la salida de la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lstWi_741l15"
      },
      "outputs": [],
      "source": [
        "def pintar_mapa(matriz_valores, titulo):\n",
        "    \"\"\"Función para pintar una matriz de valores como colores RGB.\n",
        "    Visualiza el mapa de Kohonen, donde cada neurona se representa como un rectángulo de color RGB.\n",
        "\n",
        "    Args:\n",
        "        matriz_valores: Matriz de valores RGB (lado_mapa x lado_mapa)\n",
        "        titulo: Titulo del mapa\n",
        "    \"\"\"\n",
        "\n",
        "    # Creamos una nueva figura\n",
        "    fig = plt.figure()\n",
        "\n",
        "    # Establecemos los ejes\n",
        "    ax = fig.add_subplot(111, aspect='equal') # aspect='equal' mantiene cuadrados los rectángulos\n",
        "    ax.set_xlim((0, matriz_valores.shape[0]+1)) # Limites del eje x\n",
        "    ax.set_ylim((0, matriz_valores.shape[1]+1)) # Limites del eje y\n",
        "    ax.set_title(titulo)\n",
        "\n",
        "    # Dibujamos los rectángulos por cada neurona (Un cuadrado RGB)\n",
        "    for fila in range(1, matriz_valores.shape[0] + 1):\n",
        "        for columna in range(1, matriz_valores.shape[1] + 1):\n",
        "            ax.add_patch(patches.Rectangle(\n",
        "                (fila - 0.5, columna - 0.5),                           # Posición (x,y) del rectángulo\n",
        "                1, 1,                                                  # Ancho y alto del rectángulo\n",
        "                facecolor = matriz_valores[fila - 1, columna - 1, :],  # Color RGB del rectángulo\n",
        "                edgecolor = 'none'                                     # Sin borde\n",
        "            ))\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnXfouxx1l15"
      },
      "source": [
        "## SOM Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uuV4skEQ1l15",
        "outputId": "6a21a28c-c521-4149-c64b-8c6053e1eb2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EQ: 0.4201965296790388, ET: 0.2793522267206478\n",
            "EQ: 0.3384928255796476, ET: 0.26720647773279355\n"
          ]
        }
      ],
      "source": [
        "def entrenar_datos(matriz_pesos, periodo, datos, num_datos, num_entradas, learning_rate, vecindario) -> None:\n",
        "    \"\"\"Entrena el mapa de Kohonen con los datos de entrada.\n",
        "\n",
        "    Args:\n",
        "        matriz_pesos: Matriz de pesos de las neuronas del SOM\n",
        "        periodo: Número total de iteraciones del entrenamiento\n",
        "        datos: Datos de entrada (18 x num_datos)\n",
        "        num_datos: Número de datos de entrada\n",
        "        num_entradas: Dimensionalidad de los datos de entrada (18 por pokemon)\n",
        "        learning_rate: Tasa de aprendizaje inicial\n",
        "        vecindario: Radio de influencia alrededor de la neurona ganadora\n",
        "    \"\"\"\n",
        "\n",
        "    # Entrenamiento del SOM\n",
        "    for epoca in range(periodo):\n",
        "        # Seleccionamos un patrón de entrada aleatorio\n",
        "        indice_random = np.random.randint(0, len(datos))\n",
        "        patron_azar = datos[indice_random]\n",
        "        # Buscamos la BMU para el patrón de entrada\n",
        "        neurona_bmu, bmu_indice = calcular_bmu(patron_azar, matriz_pesos, num_entradas)\n",
        "\n",
        "        # Calculamos los parámetros para esta iteración\n",
        "        lr_actual = variacion_learning_rate(learning_rate, epoca, periodo)\n",
        "        vec_actual = variacion_vecindario(vecindario, epoca, periodo)\n",
        "\n",
        "        # Actualizamos el vector de pesos de la BMU y sus vecinos\n",
        "        for fila in range(matriz_pesos.shape[0]):\n",
        "            for columna in range(matriz_pesos.shape[1]):\n",
        "                # Calculamos la distancia a la BMU\n",
        "                distancia_bmu = np.linalg.norm([fila - bmu_indice[0], columna - bmu_indice[1]])\n",
        "\n",
        "                if distancia_bmu <= vec_actual:\n",
        "                    # Calculamos la amortiguación basada en la distancia\n",
        "                    amortiguacion = decay(distancia_bmu, vec_actual)\n",
        "\n",
        "                    # Actualizamos los pesos usando la fórmula\n",
        "                    matriz_pesos[fila, columna] += lr_actual * amortiguacion * (patron_azar - matriz_pesos[fila, columna])\n",
        "\n",
        "        if epoca % 100 == 0 and epoca != 0:\n",
        "            print(f\"EQ: {error_cuantificacion(datos, matriz_pesos)}, ET: {error_topologico(datos, matriz_pesos)}\")\n",
        "\n",
        "    return matriz_pesos\n",
        "\n",
        "matriz_pesos_entrenada = entrenar_datos(matriz_pesos, periodo, datos, num_datos, num_entradas, learning_rate, vecindario)\n",
        "print(f\"EQ: {error_cuantificacion(datos, matriz_pesos_entrenada)}, ET: {error_topologico(datos, matriz_pesos_entrenada)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPbAq2Wd1l16"
      },
      "source": [
        "#### Evaluación de los parámetros\n",
        "Para evaluar los parámetros de la red, vamos a usar el conjunto de datos de entrenamiento y vamos a entrenar la red con diferentes valores de los parámetros. Vamos a usar la función de error cuadrático medio/cuadrático (MSE) y el error topológico para evaluar la calidad de la red.\n",
        "* **Error cuadrático medio (MSE)** $\\rightarrow$ Mide la diferencia entre el vector de entrada y el vector de salida de la red.\n",
        "* **Error topológico** $\\rightarrow$ Mide la calidad de la representación topológica de la red. Se calcula como el número de veces que dos vectores que están cerca en el espacio de entrada no están cerca en el mapa.\n",
        "\n",
        "Fórmula del error cuadrático medio: $$MSE = \\frac{1}{N} \\sum_{i=1}^{N} \\left \\| x_i - y_i \\right \\|^2$$\n",
        "\n",
        "Fórmula del error topológico: $$TE = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{N} d_{ij} \\cdot \\delta_{ij}$$\n",
        "\n",
        "Hemos dividido el proceso entre 2 personas para que cada uno se encargue de un rango.\n",
        "\n",
        "```python\n",
        "rangos_persona1 = {\n",
        "    'lado_mapa': [25, 50, 100],  # 3 valores\n",
        "    'periodo': list(range(1000, 5001, 1000)),\n",
        "    'learning_rate': [0.01, 0.05]\n",
        "}\n",
        "\n",
        "rangos_persona2 = {\n",
        "    'lado_mapa': [25, 50, 100],  # 3 valores\n",
        "    'periodo': list(range(1000, 5001, 1000)),\n",
        "    'learning_rate': [0.1, 0.5]\n",
        "}\n",
        "```\n",
        "\n",
        "Después de probar ambos rangos, se ha llegado a que esta combinación de valores es la más óptima:\n",
        "\n",
        "```md\n",
        "Mejores parámetros encontrados:\n",
        "* Lado del mapa: 100\n",
        "* Periodo: 5000\n",
        "* Learning rate: 0.5\n",
        "* Error de cuantificación: 0.0628282692\n",
        "* Error topológico: 0.0121457490\n",
        "```\n",
        "![Comparacion valores - Rango persona 1](./media/comparación_mejores_valores_rango_persona_1_pokemon.png)\n",
        "![Comparacion valores - Rango person a2](./media/comparación_mejores_valores_rango_persona_2_pokemon.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt28hxay1l16",
        "outputId": "4611502c-eeef-4971-a10f-7b490ed15952"
      },
      "outputs": [],
      "source": [
        "def evaluar_parametros_som(datos, num_datos, num_entradas):\n",
        "    \"\"\"Evalúa diferentes combinaciones de parámetros para encontrar los óptimos\n",
        "    usando rangos automáticos\n",
        "\n",
        "    Args:\n",
        "        datos: Datos de entrada (18 x num_datos)\n",
        "        num_datos: Número de datos de entrada\n",
        "        num_entradas: Dimensionalidad de los datos de entrada (18 por pokemon)\n",
        "\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "\n",
        "    # Definimos los rangos de parámetors que vamos a evaluar\n",
        "    rangos_parametros = {\n",
        "        'lado_mapa': [25],\n",
        "        'periodo': list(range(1000, 5001, 1000)),\n",
        "        'learning_rate': [0.01, 0.05, 0.1, 0.5]\n",
        "    }\n",
        "\n",
        "    print(\"--- Rangos a evaluar: ---\")\n",
        "    print(f\"Lados del mapa: {list(rangos_parametros['lado_mapa'])}\")\n",
        "    print(f\"Periodos: {list(rangos_parametros['periodo'])}\")\n",
        "    print(f\"Learning rates: {rangos_parametros['learning_rate']}\")\n",
        "\n",
        "    # Guardamos los resultados\n",
        "    resultados = []\n",
        "\n",
        "    #  Vamos a calcular cuantas combinaciones de parámetros vamos a probar\n",
        "    total_combinaciones = (len(list(rangos_parametros['lado_mapa'])) *\n",
        "                            len(list(rangos_parametros['periodo'])) *\n",
        "                            len(rangos_parametros['learning_rate']))\n",
        "\n",
        "    print(f\"\\n Vamos a evaluar {total_combinaciones} combinaciones de parámetros...\")\n",
        "\n",
        "    # Iteramos sobre todas las combinaciones de parámetros\n",
        "    for indice, (lado, periodo, learning_rate) in enumerate(product(\n",
        "                                                    rangos_parametros['lado_mapa'],\n",
        "                                                    rangos_parametros['periodo'],\n",
        "                                                    rangos_parametros['learning_rate']), 1):\n",
        "\n",
        "        print(f\"\\nCombinación {indice}/{total_combinaciones}\")\n",
        "        # print(f\"Lado: {lado}, Periodo: {periodo}, Learning Rate: {learning_rate}\")\n",
        "\n",
        "        # Inicializamos la matriz de pesos y el vecindario\n",
        "        matriz_pesos = np.random.random((lado, lado, num_entradas))\n",
        "        vecindario = lado // 2\n",
        "\n",
        "        # Entrenamos el SOM\n",
        "        matriz_pesos_final = matriz_pesos.copy()\n",
        "        entrenar_datos(matriz_pesos_final,\n",
        "                        periodo,\n",
        "                        datos,\n",
        "                        num_datos,\n",
        "                        num_entradas,\n",
        "                        learning_rate,\n",
        "                        vecindario)\n",
        "\n",
        "        # Calculamos el error de cuantificación\n",
        "        error_total = 0\n",
        "        for dato in range(num_datos):\n",
        "            patron = datos[dato, :]\n",
        "            _, bmu_idx = calcular_bmu(patron, matriz_pesos_final, num_entradas)\n",
        "            error = np.linalg.norm(patron - matriz_pesos_final[bmu_idx[0], bmu_idx[1]])\n",
        "            error_total += error\n",
        "\n",
        "        error_cuantificacion = error_total / num_datos\n",
        "\n",
        "        print(f\"Error medio/cuantificación: {error_cuantificacion:.6f}\")\n",
        "\n",
        "        # Calculamos el error topológico\n",
        "        errores_topologicos = 0\n",
        "        for i in range(num_datos):\n",
        "            patron = datos[i, :]\n",
        "\n",
        "            # Primera BMU\n",
        "            _, bmu1_idx = calcular_bmu(patron, matriz_pesos_final, num_entradas)\n",
        "\n",
        "            # Calculamos la segunda BMU ignorando la BMU principal\n",
        "            distancias = np.array([\n",
        "                (fila, columna, np.linalg.norm(patron - matriz_pesos_final[fila, columna]))\n",
        "                for fila in range(matriz_pesos_final.shape[0])\n",
        "                for columna in range(matriz_pesos_final.shape[1])\n",
        "                if (fila, columna) != tuple(bmu1_idx)\n",
        "            ])\n",
        "            distancias = distancias[np.argsort(distancias[:, 2])]  # Ordenamos por distancia\n",
        "            bmu2_idx = distancias[0, :2]  # Segunda BMU\n",
        "\n",
        "            # Verificamos si son adyacentes\n",
        "            distancia = np.linalg.norm(np.array(bmu1_idx) - np.array(bmu2_idx))\n",
        "            if distancia > np.sqrt(2):  # No son adyacentes\n",
        "                errores_topologicos += 1\n",
        "\n",
        "        error_topologico = errores_topologicos / num_datos\n",
        "\n",
        "        print(f\"Error topológico: {error_topologico:.6f}\")\n",
        "\n",
        "        # Guardamos resultados\n",
        "        resultados.append({\n",
        "            'lado_mapa': lado,\n",
        "            'periodo': periodo,\n",
        "            'learning_rate': learning_rate,\n",
        "            'error_cuantificacion': error_cuantificacion,\n",
        "            'error_topologico': error_topologico\n",
        "        })\n",
        "\n",
        "\n",
        "    # Normalizamos los errores para que tengan el mismo peso\n",
        "    min_cuantificacion = min(r['error_cuantificacion'] for r in resultados)\n",
        "    max_cuantificacion = max(r['error_cuantificacion'] for r in resultados)\n",
        "    min_topologico = min(r['error_topologico'] for r in resultados)\n",
        "    max_topologico = max(r['error_topologico'] for r in resultados)\n",
        "\n",
        "    def normalizar(valor, min_val, max_val):\n",
        "        return (valor - min_val) / (max_val - min_val) if max_val > min_val else 0\n",
        "\n",
        "    # Seleccionamos el mejor resultado considerando ambos errores\n",
        "    mejor_resultado = min(resultados, key=lambda x: (\n",
        "        normalizar(x['error_cuantificacion'], min_cuantificacion, max_cuantificacion) +\n",
        "        normalizar(x['error_topologico'], min_topologico, max_topologico)\n",
        "    ))\n",
        "\n",
        "    # Visualizamos los resultados\n",
        "    visualizar_resultados(resultados)\n",
        "\n",
        "    return mejor_resultado\n",
        "\n",
        "\n",
        "def visualizar_resultados(resultados):\n",
        "    \"\"\"Visualiza los resultados de la búsqueda de parámetros\n",
        "\n",
        "    Args:\n",
        "        resultados: Resultados a visualizar\n",
        "    \"\"\"\n",
        "    resultados_array = np.array([(r['lado_mapa'], r['periodo'],\n",
        "                                r['learning_rate'], r['error_cuantificacion'],\n",
        "                                r['error_topologico'])\n",
        "                                for r in resultados])\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Gráfico 1: Lado vs Error Cuantificación\n",
        "    ax1 = fig.add_subplot(231)\n",
        "    for lr in np.unique(resultados_array[:, 2]):\n",
        "        mask = resultados_array[:, 2] == lr\n",
        "        ax1.plot(resultados_array[mask, 0],\n",
        "                resultados_array[mask, 3],\n",
        "                'o-',\n",
        "                label=f'LR={lr}')\n",
        "    ax1.set_xlabel('Lado del Mapa')\n",
        "    ax1.set_ylabel('Error de Cuantificación')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Gráfico 2: Periodo vs Error Cuantificación\n",
        "    ax2 = fig.add_subplot(232)\n",
        "    for lr in np.unique(resultados_array[:, 2]):\n",
        "        mask = resultados_array[:, 2] == lr\n",
        "        ax2.plot(resultados_array[mask, 1],\n",
        "                resultados_array[mask, 3],\n",
        "                'o-',\n",
        "                label=f'LR={lr}')\n",
        "    ax2.set_xlabel('Periodo')\n",
        "    ax2.set_ylabel('Error de Cuantificación')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Gráfico 3: Learning Rate vs Error Cuantificación\n",
        "    ax3 = fig.add_subplot(233)\n",
        "    for lado in np.unique(resultados_array[:, 0]):\n",
        "        mask = resultados_array[:, 0] == lado\n",
        "        ax3.plot(resultados_array[mask, 2],\n",
        "                resultados_array[mask, 3],\n",
        "                'o-',\n",
        "                label=f'Lado={int(lado)}')\n",
        "    ax3.set_xlabel('Learning Rate')\n",
        "    ax3.set_ylabel('Error de cuantificación')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "    # Gráfico 4: Lado vs Error Topológico\n",
        "    ax4 = fig.add_subplot(234)\n",
        "    for lr in np.unique(resultados_array[:, 2]):\n",
        "        mask = resultados_array[:, 2] == lr\n",
        "        ax4.plot(resultados_array[mask, 0],\n",
        "                resultados_array[mask, 4],\n",
        "                'o-',\n",
        "                label=f'LR={lr}')\n",
        "    ax4.set_xlabel('Lado del Mapa')\n",
        "    ax4.set_ylabel('Error Topológico')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True)\n",
        "\n",
        "    # Gráfico 5: Periodo vs Error Topológico\n",
        "    ax5 = fig.add_subplot(235)\n",
        "    for lr in np.unique(resultados_array[:, 2]):\n",
        "        mask = resultados_array[:, 2] == lr\n",
        "        ax5.plot(resultados_array[mask, 1],\n",
        "                resultados_array[mask, 4],\n",
        "                'o-',\n",
        "                label=f'LR={lr}')\n",
        "    ax5.set_xlabel('Periodo')\n",
        "    ax5.set_ylabel('Error Topológico')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True)\n",
        "\n",
        "    # Gráfico 6: Learning Rate vs Error Topológico\n",
        "    ax6 = fig.add_subplot(236)\n",
        "    for lado in np.unique(resultados_array[:, 0]):\n",
        "        mask = resultados_array[:, 0] == lado\n",
        "        ax6.plot(resultados_array[mask, 2],\n",
        "                resultados_array[mask, 4],\n",
        "                'o-',\n",
        "                label=f'Lado={int(lado)}')\n",
        "    ax6.set_xlabel('Learning Rate')\n",
        "    ax6.set_ylabel('Error Topológico')\n",
        "    ax6.legend()\n",
        "    ax6.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#mejor_resultado = evaluar_parametros_som(datos, num_datos, num_entradas)\n",
        "\n",
        "# print(\"\\nMejores parámetros encontrados:\")\n",
        "# print(f\"Lado del mapa: {mejor_resultado['lado_mapa']}\")\n",
        "# print(f\"Periodo: {mejor_resultado['periodo']}\")\n",
        "# print(f\"Learning rate: {mejor_resultado['learning_rate']}\")\n",
        "# print(f\"Error de cuantificación: {mejor_resultado['error_cuantificacion']:.10f}\")\n",
        "# print(f\"Error topológico: {mejor_resultado['error_topologico']:.10f}\")\n",
        "\n",
        "# # Guardamos los mejores parámetros para su uso posterior\n",
        "# parametros_optimos = {\n",
        "#      'lado_mapa': mejor_resultado['lado_mapa'],\n",
        "#      'periodo': mejor_resultado['periodo'],\n",
        "#      'learning_rate': mejor_resultado['learning_rate'],\n",
        "#      'vecindario': mejor_resultado['lado_mapa'] // 2\n",
        "# }\n",
        "\n",
        "parametros_optimos = {\n",
        "    'lado_mapa': 100,\n",
        "    'periodo': 5000,\n",
        "    'learning_rate': 0.5,\n",
        "    'vecindario': 100 // 2\n",
        "}\n",
        "\n",
        "matriz_pesos_inicial = np.random.random((parametros_optimos['lado_mapa'], parametros_optimos['lado_mapa'], num_entradas))\n",
        "matriz_pesos_mejores_parametros = entrenar_datos(matriz_pesos_inicial, parametros_optimos['periodo'], datos, num_datos, num_entradas, parametros_optimos['learning_rate'], parametros_optimos['vecindario'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rJMpgJO1l16"
      },
      "source": [
        "## SOM Clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez que hemos entrenado nuestro SOM, podemos usarlo para clasificar nuevos datos asignándolos a las neuronas que mejor representan sus características. Este proceso se basa en buscar la BMU para cada nuevo dato (como en el entrenamiento) y asignarle la etiqueta de la neurona ganadora.\n",
        "\n",
        "Con un nuevo vector de entrada, la clasificación se hace de la siguiente forma:\n",
        "1. **Cálculo de la BMU** $\\rightarrow$ Buscamos la neurona con el vector de pesos más cercano al vector de entrada, minimizando la distancia euclídea.\n",
        "2. **Asignación de la clase** $\\rightarrow$ Asignamos la clase de la BMU al dato de entrada.\n",
        "3. **Visualización** $\\rightarrow$ Podemos construir diferentes mapas para analizar el comportamiento de la clasificación y la estructura de los datos.\n",
        "\n",
        "El número de clases hace referencia a la cantidad de diferentes categorías que han sido asignadas/identificadas. Las clases nos permiten verificar si el SOM ha identificado correctamente la cantidad esperada de categorías en los datos y en problemas sin etiquetas, nos ayuda a entender la estructura de los datos y ver patrones ocultos. \n",
        "* En un problema supervisado, el número de clases es igual al número de etiquetas.\n",
        "* En un problema no supervisado, el número de clases es igual al número de grupos identificados por el SOM.\n",
        "\n",
        "Como hemos explicado anteriormente, después del entrenamiento y la clasificación, podemos  generar diferentes mapas de análisis/salida para interpretar los resultados.\n",
        "* **Mapa de clasificación** $\\rightarrow$ Muestra la distribución de las clases asignadas en el mapa. \n",
        "  * Cada neurona se colorea según la clase mayoritaria de los datos que representa.\n",
        "* **Mapa de activaciones** $\\rightarrow$ Este mapa nos muesta cuántos datos han sido asignados a cada neurona. \n",
        "  * Es útil para ver cómo de bien se ha ajustado el SOM a los datos de entrada. \n",
        "  * Cada celda del mapa representa una neurona y su activación se mide por la cantidad de veces que fue seleccionada como BMU.\n",
        "  * Zonas con muchas activaciones indican que la neurona representa bien los datos.\n",
        "  * Zonas con pocas activaciones pueden sugerir regiones subutilizadas o datos poco representativos.\n",
        "* **Mapa de distancias** $\\rightarrow$ Representa la distancia media entre los pesos de una neurona y sus vecinas.\n",
        "  * Zonas de baja distancia indican grupos de neuronas similares.\n",
        "  * Zonas de alta distancia indican fronteras entre grupos.\n",
        "  * Es útil para ver e identificar fronteras naturales entre categorías.\n",
        "  * Nos ayuda a ver si los datos est'an bien separados o si hay regiones de transición."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "451cGfaH1l16"
      },
      "outputs": [],
      "source": [
        "tipo_color_dict = {\n",
        "    'water': '#1E90FF',    # Azul\n",
        "    'fire': '#FF4500',     # Rojo\n",
        "    'grass': '#32CD32',    # Verde\n",
        "    'electric': '#FFD700', # Amarillo\n",
        "    'bug': '#9ACD32',      # Verde amarillento\n",
        "    'dark': '#4B0082',     # Índigo\n",
        "    'dragon': '#8A2BE2',   # Violeta\n",
        "    'fairy': '#FF69B4',    # Rosa\n",
        "    'fight': '#B22222',    # Rojo oscuro\n",
        "    'flying': '#87CEEB',   # Azul claro\n",
        "    'ghost': '#6A5ACD',    # Azul violáceo\n",
        "    'ground': '#D2691E',   # Marrón\n",
        "    'ice': '#00FFFF',      # Cian\n",
        "    'normal': '#C0C0C0',   # Gris\n",
        "    'poison': '#800080',   # Púrpura\n",
        "    'psychic': '#FF00FF',  # Fucsia\n",
        "    'rock': '#A52A2A',     # Marrón roca\n",
        "    'steel': '#B0C4DE',    # Azul acero\n",
        "}\n",
        "\n",
        "def mezclar_colores_segun_tipo(color1, color2):\n",
        "    \"\"\"Mezcla los colores asignados a cada tipo para conseguir un color intermedio\n",
        "\n",
        "    Args:\n",
        "        color1: Color asociado al tipo 1\n",
        "        color2: Color asociado al tipo 2\n",
        "\n",
        "    Returns:\n",
        "        mcolors.to_hex(rgb_medio): Utilizando la libreria mcolors, devolvemos el color intermedio\n",
        "    \"\"\"\n",
        "    rgb1 = np.array(mcolors.to_rgb(color1))\n",
        "    rgb2 = np.array(mcolors.to_rgb(color2))\n",
        "    rgb_medio = (rgb1 + rgb2) / 2\n",
        "    return mcolors.to_hex(rgb_medio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut1IIm3I1l16",
        "outputId": "2fa52ec5-1fc8-4575-e1cd-fcbbe3dcfadd"
      },
      "outputs": [],
      "source": [
        "def clasificar_som(datos, matriz_pesos, calcular_bmu):\n",
        "    \"\"\"Clasifica un dataset de patrones usando una matriz de pesos entrenada.\n",
        "\n",
        "    Args:\n",
        "        datos: Datos de entrada (num_entradas x num_patrones)\n",
        "        matriz_pesos: Matriz de pesos entrenada (lado_mapa x lado_mapa x num_entradas)\n",
        "        calcular_bmu: Función para calcular la BMU\n",
        "\n",
        "    Returns:\n",
        "        tuple (mapa_clasificacion, mapa_activaciones, mapa_distancias, num_clases, error_cuantificacion, error_topologico):\n",
        "            - mapa_clasificacion: Matriz con los patrones clasificados por cada neurona (lado_mapa x lado_mapa x num_entradas)\n",
        "            - mapa_activaciones: Matriz con el número de patrones reconocidos por cada neurona (lado_mapa x lado_mapa)\n",
        "            - mapa_distancias: Matriz con la distancia media de los patrones con el vector de pesos (lado_mapa x lado_mapa)\n",
        "            - num_clases: Número de clases identificadas\n",
        "            - error_cuantificacion: Error de cuantificación\n",
        "            - error_topologico: Error topológico\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtenemos las dimensiones de la matriz de pesos\n",
        "    # y los patrones de entrada\n",
        "    filas = matriz_pesos.shape[0]\n",
        "    columnas = matriz_pesos.shape[1]\n",
        "    dim_entrada = matriz_pesos.shape[2]\n",
        "    num_patrones = datos.shape[0]  # Número de patrones (segunda dimensión de datos)\n",
        "\n",
        "    # 1. Definimos las tres matrices y las inicializamos a 0\n",
        "    # Mapa de clasificación: Guarda el último patrón clasificado en cada neurona\n",
        "    mapa_clasificacion = np.full((lado_mapa, lado_mapa), ' ')\n",
        "\n",
        "    # Mapa de activaciones: Guarda el número de patrones reconocidos por cada neurona\n",
        "    mapa_activaciones = np.zeros([filas, columnas])\n",
        "\n",
        "    # Mapa de distancias: Guarda la distancia media de los patrones con el vector de pesos\n",
        "    mapa_distancias = np.zeros([filas, columnas])\n",
        "\n",
        "    # Sumas totales de cada error\n",
        "    suma_error_cuantificacion = 0\n",
        "    suma_error_topologico = 0\n",
        "    # Vector que contendrá todos los tipos encontrados en el dataset.\n",
        "    leyenda_tipos = {}\n",
        "\n",
        "    # 2. Recorremos todo el dataset de patrones de entrada\n",
        "    for i,patron in enumerate(datos):\n",
        "        # Obtenemos el patrón actual para clasificar\n",
        "        patron_actual = patron\n",
        "\n",
        "        # Calculamos la BMU para el patrón actual\n",
        "        vector_bmu, coords_bmu = calcular_bmu(patron_actual, matriz_pesos_entrenada, dim_entrada)\n",
        "        fila_bmu, col_bmu = coords_bmu\n",
        "        tipo1, tipo2 = tipos_pokemon[i]\n",
        "\n",
        "        # Unificamos los tipos en uno solo, sabiendo que tipo2 puede estar vacío.\n",
        "        tipo_completo = f'{tipo1}/{tipo2}' if tipo2 != '-' else tipo1\n",
        "\n",
        "        # Asignamos un color lógico (por ej. agua-azul) a cada tipo encontrado.\n",
        "        color1 = tipo_color_dict.get(tipo1, \"#000000\")\n",
        "        color2 = tipo_color_dict.get(tipo2, color1)\n",
        "        # Mezclamos los colores para que el tipo_completo sea de un color que tenga sentido teniendo en cuenta ambos tipos\n",
        "        # y teniendo en cuenta que el tipo 1 es el predominante.\n",
        "        color = mezclar_colores_segun_tipo(color1, color2) if tipo1 != tipo2 else color1\n",
        "\n",
        "        #Vamos creando pokemon a pokemon el mapa de classificación.\n",
        "        plt.scatter(fila_bmu, col_bmu, color=color, edgecolors='black', s=100)\n",
        "        if tipo_completo not in leyenda_tipos:\n",
        "            leyenda_tipos[tipo_completo] = color\n",
        "        # Calculamos la distancia entre el patrón actual y la BMU actual (error de cuantificación)\n",
        "        distancia_patron_bmu = np.linalg.norm(matriz_pesos[int(fila_bmu), int(col_bmu)] - patron_actual)**2\n",
        "\n",
        "        # Actualizamos el mapa de clasificación con el patrón actual\n",
        "        mapa_clasificacion[fila_bmu, col_bmu] = tipo_completo\n",
        "\n",
        "        # Incrementamos el contador de activaciones de la BMU\n",
        "        mapa_activaciones[int(fila_bmu), int(col_bmu)] += 1\n",
        "\n",
        "        # Añadimos la distancia entre el patrón y la BMU al mapa de distancias\n",
        "        mapa_distancias[int(fila_bmu), int(col_bmu)] = (\n",
        "            (mapa_distancias[int(fila_bmu), int(col_bmu)] * (mapa_activaciones[int(fila_bmu), int(col_bmu)] - 1) + distancia_patron_bmu) /\n",
        "            mapa_activaciones[int(fila_bmu), int(col_bmu)]\n",
        "        )\n",
        "\n",
        "        # Añadimos el error de cuantificación al total\n",
        "        suma_error_cuantificacion += distancia_patron_bmu\n",
        "\n",
        "        # Calculamos el error topológico\n",
        "        # Calculamos las distancias a todas las neuronas para el patrón actual\n",
        "        distancias = np.zeros([filas, columnas])\n",
        "        for fila in range(filas):\n",
        "            for columna in range(columnas):\n",
        "                distancias[fila, columna] = np.linalg.norm(matriz_pesos[fila, columna] - patron_actual)**2\n",
        "\n",
        "        # Marcamos la BMU como infinito para encontrar la segunda BMU\n",
        "        # Encontramos la segunda BMU utilizando la misma estrategia que en Forma 1\n",
        "        distancias_lista = np.array([\n",
        "            (fila, columna, np.linalg.norm(matriz_pesos[fila, columna] - patron_actual))\n",
        "            for fila in range(filas)\n",
        "            for columna in range(columnas)\n",
        "            if (fila, columna) != (fila_bmu, col_bmu)  # Excluir la BMU principal\n",
        "        ])\n",
        "        distancias_lista = distancias_lista[np.argsort(distancias_lista[:, 2])]  # Ordenar por distancia\n",
        "        bmu2_idx = distancias_lista[0, :2]  # Segunda BMU más cercana\n",
        "        fila_bmu2, col_bmu2 = int(bmu2_idx[0]), int(bmu2_idx[1])\n",
        "\n",
        "        # Comprobamos si las dos BMUs son adyacentes\n",
        "        if (abs(fila_bmu - fila_bmu2) <= 1 and abs(col_bmu - col_bmu2) <= 1):\n",
        "            # Son adyacentes - no hay error topológico\n",
        "            suma_error_topologico += 0\n",
        "        else:\n",
        "            # No son adyacentes - hay error topológico\n",
        "            suma_error_topologico += 1\n",
        "\n",
        "    # Calculamos el número de clases (neuronas activadas)\n",
        "    num_clases = np.count_nonzero(mapa_activaciones)\n",
        "\n",
        "    # Calculamos los errores medios\n",
        "    error_cuantificacion = suma_error_cuantificacion / num_patrones\n",
        "    error_topologico = suma_error_topologico / num_patrones\n",
        "\n",
        "    return leyenda_tipos, mapa_clasificacion, mapa_activaciones, mapa_distancias, num_clases, error_cuantificacion, error_topologico\n",
        "\n",
        "leyenda_tipos, mapa_clasificacion, mapa_activaciones, mapa_distancias, num_clases, error_cuantificacion, error_topologico = clasificar_som(datos, matriz_pesos_entrenada, calcular_bmu)\n",
        "\n",
        "# 1. Mostramos el mapa de clasificación\n",
        "tipos_ordenados = sorted(leyenda_tipos.items(), key=lambda x: mcolors.to_rgb(x[1]))  # Ordena según el valor RGB del color\n",
        "\n",
        "# Crear la leyenda con las entradas ordenadas\n",
        "leyenda_puntos = [Line2D([0], [0], marker='o', color='w', markerfacecolor=col, markersize=10, label=tipo)\n",
        "                    for tipo, col in tipos_ordenados]\n",
        "\n",
        "# Mostrar la leyenda con dos columnas\n",
        "plt.legend(handles=leyenda_puntos, title=\"Tipos de Pokémon\", bbox_to_anchor=(1.05, 1), loc='upper left', ncol=3)\n",
        "plt.xlim(0, 100)\n",
        "plt.ylim(0, 100)\n",
        "plt.xlabel('Coordenada X')\n",
        "plt.ylabel('Coordenada Y')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Mostramos el histograma 3D del Mapa de Activaciones\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "x = np.arange(mapa_activaciones.shape[0])\n",
        "y = np.arange(mapa_activaciones.shape[1])\n",
        "X, Y = np.meshgrid(x, y)\n",
        "ax.bar3d(X.ravel(), Y.ravel(), np.zeros_like(mapa_activaciones).ravel(),\n",
        "            1, 1, mapa_activaciones.ravel(), shade=True)\n",
        "ax.set_title(\"Mapa de Activaciones (Histograma 3D)\")\n",
        "ax.set_xlabel(\"X\")\n",
        "ax.set_ylabel(\"Y\")\n",
        "ax.set_zlabel(\"Frecuencia\")\n",
        "ax.set_zlabel(\"Frecuencia\", labelpad=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Mostramos el mapa de distancias\n",
        "plt.figure()\n",
        "plt.title(\"Mapa de Distancias del Dataset\")\n",
        "im = plt.imshow(mapa_distancias, cmap='hot_r')\n",
        "plt.colorbar(im, label=\"Distancia Media\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. Número de clases y errores\n",
        "print(f\"Número de clases identificadas: {num_clases}\")\n",
        "print(f\"Error de Cuantificación: {error_cuantificacion:.6f}\")\n",
        "print(f\"Error Topológico: {error_topologico:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjSDHgYp1l16"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjzWOeKd1l16"
      },
      "source": [
        "## SOM Prueba\n",
        "\n",
        "El proceso de prueba es la fase en la que evaluamos cómo se comporta la red con datos nuevos después del entrenamiento. En esta fase, no actualizamos los pesos, si no que analizamos cómo la red clasifica los nuevos datos.\n",
        "\n",
        "Los objetivos de la fase de prueba son:\n",
        "* **Evaluar la calidad de la clasificación** $\\rightarrow$ Verificar si la red ha aprendido correctamente las características de los datos y es capaz de generalizar y organizar nuevos datos.\n",
        "* **Observar la estructura de los datos** $\\rightarrow$ Ver cómo los patrones de entrada se asignan a las neuronas y cómo se organizan en el mapa.\n",
        "* **Identificar errores** $\\rightarrow$ Ver si hay datos mal clasificados y entender por qué la red ha cometido errores utilizando el error de cuantificación y el error topológico.\n",
        "\n",
        "Como hemos mencionado anteriormente, en la fase de prueba no se actualizan los pesos del SOM. Los pasos a seguir son:\n",
        "1. **Presentación de nuevos datos** $\\rightarrow$ Presentamos un conjunto nuevo de datos que el SOM no ha visto durante el entrenamiento. Cada dato de entrada es un vector de características similares a los datos de entrenamiento.\n",
        "2. **Cálculo de la BMU** $\\rightarrow$ Para cada dato de entrada, buscamos la mejor neurona (BMU) que representa mejor las características del dato.\n",
        "3. **Asignación de la clase** $\\rightarrow$ Asignamos qué neurona del SOM ha reconocido cada dato. \n",
        "4. **Análisis de los resultados** $\\rightarrow$ Analizamos cómo se han clasificado los datos y si hay errores mediante el error topológico (indica si los datos similares se asignan a neuronas adyacentes) y el mapa de clasificación (muestra dónde se han asignado los datos de prueba en el som).\n",
        "\n",
        "Las diferencias entre las fases de entrenamiento, clasificación y prueba son las siguientes:\n",
        "\n",
        "| **Fase**                        | **SOM de Entrenamiento**            | **SOM de Clasificación**                    | **SOM de Prueba**                       |\n",
        "|---------------------------------|-------------------------------------|---------------------------------------------|-----------------------------------------|\n",
        "| **Se le muestran datos nuevos** | No (usa datos de entrenamiento)     | No (usa los mismos datos del entrenamiento) | Sí (se prueban datos no vistos)         |\n",
        "| **Modifica pesos**              | Sí (ajusta los pesos)               | No (solo asigna patrones)                   | No (solo analiza datos nuevos)          |\n",
        "| **Encuentra BMU**               | Sí                                  | Sí                                          | Sí                                      |\n",
        "| **Ajusta la vecindad**          | Sí (disminuye con el tiempo)        | No                                          | No                                      |\n",
        "| **Calcula errores**             | Sí (para optimizar el aprendizaje)  | Sí (para evaluar la clasificación)          | Sí (para medir la generalización)       |\n",
        "| **Objetivo**                    | Aprender la estructura de los datos | Organizar datos entrenados                  | Evaluar el rendimiento con datos nuevos |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "p2ZpZE9O1l16",
        "outputId": "8b01c401-4c5e-4156-81da-98dfe4a07f4e"
      },
      "outputs": [],
      "source": [
        "# Clasifica nuevos patrones (pokemon_classify)\n",
        "\n",
        "dataset_clasi = \"data/pokemon_classify.csv\"\n",
        "datos_clasificar_original = pd.read_csv(dataset_clasi)\n",
        "\"\"\"Contiene todos los datos de clasificacion de nuevos Pokemons\"\"\"\n",
        "\n",
        "tipos_pokemon = datos_clasificar_original[['type1', 'type2']].fillna('-').to_numpy()\n",
        "\n",
        "columnas_relevantes = [col for col in datos_original.columns if \"against_\" in col]\n",
        "datos_clasificar = datos_clasificar_original[columnas_relevantes].values\n",
        "\"\"\"Filtramos obteniendo únicamente las columnas de cómo reaccionan ante los ataques de los tipos de Pokemon\"\"\"\n",
        "\n",
        "valor_maximo = datos_clasificar.max(axis=0)\n",
        "\"\"\"Valor máximo de los datos de entrada\"\"\"\n",
        "\n",
        "if normalizar_datos:\n",
        "    # Escalamos los datos al rango [0, 1]\n",
        "    datos_clasificar = datos_clasificar / np.max(valor_maximo)\n",
        "\n",
        "leyenda_tipos, mapa_clasificacion, mapa_activaciones, mapa_distancias, num_clases, error_cuantificacion, error_topologico = clasificar_som(datos_clasificar, matriz_pesos_entrenada, calcular_bmu)\n",
        "\n",
        "# 3. Mostramos el mapa de clasificación\n",
        "tipos_ordenados = sorted(leyenda_tipos.items(), key=lambda x: mcolors.to_rgb(x[1]))  # Ordena según el valor RGB del color\n",
        "\n",
        "# Crear la leyenda con las entradas ordenadas\n",
        "leyenda_puntos = [Line2D([0], [0], marker='o', color='w', markerfacecolor=col, markersize=10, label=tipo)\n",
        "                  for tipo, col in tipos_ordenados]\n",
        "\n",
        "# Mostrar la leyenda con dos columnas\n",
        "plt.legend(handles=leyenda_puntos, title=\"Tipos de Pokémon\", bbox_to_anchor=(1.05, 1), loc='upper left', ncol=3)\n",
        "\n",
        "plt.xlabel('Coordenada X')\n",
        "plt.ylabel('Coordenada Y')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 4. Mostramos el histograma 3D del Mapa de Activaciones\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "x = np.arange(mapa_activaciones.shape[0])\n",
        "y = np.arange(mapa_activaciones.shape[1])\n",
        "X, Y = np.meshgrid(x, y)\n",
        "ax.bar3d(X.ravel(), Y.ravel(), np.zeros_like(mapa_activaciones).ravel(),\n",
        "            1, 1, mapa_activaciones.ravel(), shade=True)\n",
        "ax.set_title(\"Mapa de Activaciones (Histograma 3D)\")\n",
        "ax.set_xlabel(\"X\")\n",
        "ax.set_ylabel(\"Y\")\n",
        "ax.set_zlabel(\"Frecuencia\")\n",
        "ax.set_zlabel(\"Frecuencia\", labelpad=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5. Mostramos el mapa de distancias\n",
        "plt.figure()\n",
        "plt.title(\"Mapa de Distancias del Dataset\")\n",
        "im = plt.imshow(mapa_distancias, cmap='hot_r')\n",
        "plt.colorbar(im, label=\"Distancia Media\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. Número de clases y errores\n",
        "print(f\"Número de clases identificadas: {num_clases}\")\n",
        "print(f\"Error de Cuantificación: {error_cuantificacion:.6f}\")\n",
        "print(f\"Error Topológico: {error_topologico:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos observar, la red ha clasificado de forma efectiva los 4 pokemos del data set de clasificación. \n",
        "- Pikachu se clasifica en el tipo electric en la zona en la que cayeron los pokemons de tipo eléctrico en el apartado de clasificación\n",
        "- Articuno y Moltres pertenecen a la misma zona por dos motivos. El primero es que sus datos de daño recibido por otros tipos son bastante similares, con alguna diferencia, pero mínima: \n",
        "    - Articuno: 0.5,1.0,1.0,2.0,1.0,1.0,2.0,1.0,1.0,0.5,0.0,1.0,1.0,1.0,1.0,4.0,2.0,1.0\n",
        "    - Moltres: 0.25,1.0,1.0,2.0,0.5,0.5,0.5,1.0,1.0,0.25,0.0,1.0,1.0,1.0,1.0,4.0,0.5,2.0\n",
        "    - BMU asignada a los dos: \n",
        "   Por otro lado, la red clasifica a ambos en el mismo grupo porque ambos tienen el mismo tipo2, y aunque previamente hemos establecido que el tipo1 es el primario, el que sean ambos de 'flying' influye lo suficiente como para que estén en la misma clase. \n",
        "- Slowbro está clasificado cerca del grupo de water/psyquic que se formo en el apartado de Clasificación. Vemos en ese mapa que está cerca de todos los tipos relacionados con water y psyquic."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IA_II",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
