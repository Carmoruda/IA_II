{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 3 - CNN para clasificar imágenes de frutas - Lab 1\n",
    "## Preparación de entorno\n",
    "#### Instalar las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy\n",
    "# %pip install pandas\n",
    "# %pip install matplotlib\n",
    "# %pip install torch\n",
    "# %pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar librerías de código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN SetUp\n",
    "* **Red convolucional secuencial (CNN secuencial)** $\\rightarrow$ Tipo de red neuronal convolucional diseñada utilizando un modelo secuenncial (una pila lineal de capas donde cada capa recibe la salida de la capa anterior como entrada).\n",
    "  * **Convolución** $\\rightarrow$ Operación matemática que combina dos funciones para producir una tercera función. En el contexto de las CNN, se utiliza para extraer características de las imágenes.\n",
    "  * **Capas** $\\rightarrow$ Capas de convolución, activación, pooling y completamente conectadas (fully connected).\n",
    "  * Recibe **varias entradas** (como señales), cada una con un peso que indica su importancia.\n",
    "\n",
    "* Componentes principales de una CNN secuencial:\n",
    "  * **Capas convolucionales** $\\rightarrow$ Aplican filtros para extraer características relevantes de las imágenes, como bordes, texturas o patrones. El tamaño y número de filtros son hiperparámetros que definimos nosotros.\n",
    "  * **Funciones de activación** $\\rightarrow$ Generalmente, se utiliza *ReLU* para introducir no linealidad en las capas intermedias, mientras que *Softmax* se utiliza en la capa de salida para problemas de clasificación multiclase.\n",
    "  * **Capas de pooling** $\\rightarrow$ Reducen las dimensiones espaciales (alto y ancho) de las características manteniendo las más relevantes. Esto disminuye el coste computacional y nos ayuda a prevenir el sobreajuste.\n",
    "  * **Capas completamente conectadas** $\\rightarrow$ Al final, las características que hemos extraído las aplanamos y las pasamos a una o más capas densas (completamente conectadas) para hacer la clasificación final.\n",
    "  * **Optimizador y fución de pérdida** $\\rightarrow$ Utilizamos *RMSprop* como optimizador y *categorical_crossentropy* como función de pérdida para problemas de clasificación multiclase. Estos son hiperparámetros que podemos ajustar según nuestras necesidades.\n",
    "\n",
    "<img src=\"./media/Estructura cnn.jpg\" width=\"70%\" style=\"display: block; margin: 0 auto;\"/>\n",
    "\n",
    "<img src=\"./media/Arquitectura cnn.jpg\" width=\"70%\" style=\"display: block; margin: 0 auto; padding-top: 15px;\"/>\n",
    "\n",
    "* Aplicaciones de las redes convolucionales:\n",
    "  * **Clasificación de imágenes** $\\rightarrow$ Identificar objetos en imágenes, como en nuestro caso (clasificar frutas).\n",
    "  * **Detección de objetos** $\\rightarrow$ Localizar y clasificar múltiples objetos en una imagen.\n",
    "  * **Segmentación de imágenes** $\\rightarrow$ Dividir una imagen en regiones significativas, como identificar diferentes partes de una imagen médica.\n",
    "  * **Reconocimiento facial** $\\rightarrow$ Identificar y verificar caras en imágenes o videos.\n",
    "  * **Procesamiento de video** $\\rightarrow$ Analizar secuencias temporales como en coches autónomos o vigilancia.\n",
    "  * **Generación de imágenes** $\\rightarrow$ Crear imágenes nuevas a partir de datos existentes, como en el caso de *GANs* (Generative Adversarial Networks).\n",
    "  * **Reconocimietnto de texto** $\\rightarrow$ Extraer texto de imágenes (OCR).\n",
    "  \n",
    "### Implementación\n",
    "\n",
    "Lo primero que vamos a hacer antes de entrenar la red, es preparar adecuadamente los datos. En este caso, vamos a utilizar el dataset de frutas [*\"Fruits 360\"*](https://www.kaggle.com/datasets/moltean/fruits/data), donde las imágenes están organizadas en carpetas según su clase (es decir, cada carpeta corresponde a una fruta distinta). Esto nos va a ayudar mucho, porque podemos utlizar la función *ImageFolder* de PyTorch, que automáticamente asigna una etiqueta a cada imagen según el nombre de la carpeta en la que se encuentra.\n",
    "\n",
    "Como las redes neuronales no pueden procesar directamente JPEGs, necesitamos convertirlas a tensores, que son estructuras numéricas similares a matrices. Para ello vamos a usar una serie de transformaciones que aplicarmeros a cada imagen: primero las redimensionamos a un tamaño uniforme (64x64 píxeles), y luego las convertimos a tensores, lo cual también normaliza sus valores de píxel entre 0 y 1. Esto va ayudar a que nuestro modelo entrene de forma más estable.\n",
    "\n",
    "> **Nota:** Cuanto más grande sea la imagen, más tiempo tardará en entrenar el modelo. En este caso, hemos elegido 64x64 píxeles como un tamaño intermedio que debería funcionar bien para nuestro problema. En general, es recomendable usar imágenes de tamaño uniforme para evitar problemas de memoria y mejorar la eficiencia del entrenamiento. Sin embargo, si tenemos imágenes de diferentes tamaños, podemos usar técnicas de *data augmentation* para aumentar la diversidad del dataset y mejorar la capacidad de generalización del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas de los directorios de datos\n",
    "DIRECTORIO_ENTRENAMIENTO = './data/FandV/Training'\n",
    "DIRECTORIO_PRUEBAS = './data/FandV/Test'\n",
    "\n",
    "# Transformaciones que vamos a aplicar a las imágenes\n",
    "transformacion = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),       # Redimensionamos todas las imágenes al mismo tamaño (64x64)\n",
    "    transforms.ToTensor(),             # Convertimos la imagen a tensor (valores entre 0 y 1)\n",
    "])\n",
    "\n",
    "# Cargamos las imágenes usando ImageFolder, que usa la carpeta como etiqueta\n",
    "dataset = datasets.ImageFolder(DIRECTORIO_ENTRENAMIENTO, transform=transformacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que ya hemos cagado todas las imágenes y las tenemos en el tensor, tenemos que dividir el conjunto de datos de `DIRECTORIO_ENTRENAMIENTO` en dos subconjuntos:\n",
    "* **Conjunto de entrenamiento** $\\rightarrow$ Lo vamos a utilizar para entrenar el modelo. Este conjunto tiene la mayoría de las imágenes y es donde el modelo aprende a reconocer patrones y características de las frutas.\n",
    "* **Conjunto de validación** $\\rightarrow$ Lo utilizamos para evaluar el rendimiento del modelo durante el entrenamiento. Este conjunto tiene menos imágenes y se utiliza para comprobar si el modelo está aprendiendo correctamente y no se está sobreajustando a los datos de entrenamiento.\n",
    "\n",
    "Esta división nos va a permitir entrenar el modelo con una parte de los latos y luego comprobar como de bien generaliza con datos que no ha visto antes. Normalmente, se suele reservar un 20% de los datos para validación. Para dividir el dataset, vamos a utilizar la función `random_split` de PyTorch, que nos permite dividir un dataset en dos subconjuntos de forma aleatoria. Esta función toma como entrada el dataset original y las longitudes de los subconjuntos que queremos crear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "                             Información del dataset                             \n",
      "---------------------------------------------------------------------------------\n",
      "* Directorio de entrenamiento: ./data/FandV/Training\n",
      "* Directorio de pruebas: ./data/FandV/Test\n",
      "\n",
      "\n",
      "* Número total de imágenes: 92545\n",
      "* Número de imágenes de entrenamiento: 74036\n",
      "* Número de imágenes de validación: 18509\n",
      "* Dimensiones de las imágenes: torch.Size([3, 64, 64])\n",
      "\n",
      "\n",
      "* Número de batches de entrenamiento: 2314\n",
      "* Número de batches de validación: 579\n",
      "* Tamaño de cada batch de entrenamiento: 32\n",
      "* Tamaño de cada batch de validación: 32\n",
      "\n",
      "\n",
      "* Número de clases: 180\n",
      "* Ejemplo de clases: ['Apricot 1', 'Avocado 1', 'Avocado ripe 1', 'Banana 1']\n"
     ]
    }
   ],
   "source": [
    "# Dividimos el dataset en entrenamiento y validacion (80%-20%)\n",
    "tamano_entrenamiento = int(0.8 * len(dataset))\n",
    "tamano_validacion = len(dataset) - tamano_entrenamiento\n",
    "\n",
    "dataset_entrenamiento, dataset_validacion = random_split(dataset, [tamano_entrenamiento, tamano_validacion])\n",
    "\n",
    "# Usamos DataLoader para cargar los datos en batches\n",
    "# (batch_size=32 significa que cargamos 32 imágenes a la vez)\n",
    "# (shuffle=True significa que mezclamos los datos en cada época)\n",
    "loader_entrenamiento = DataLoader(dataset_entrenamiento, batch_size=32, shuffle=True)\n",
    "loader_validacion = DataLoader(dataset_validacion, batch_size=32, shuffle=False)\n",
    "\n",
    "# Número de clases (etiquetas diferentes)\n",
    "num_classes = len(dataset.classes)\n",
    "\n",
    "# Mostramos la información del dataset\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "print(\"                             Información del dataset                             \")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "print(f\"* Directorio de entrenamiento: {DIRECTORIO_ENTRENAMIENTO}\")\n",
    "print(f\"* Directorio de pruebas: {DIRECTORIO_PRUEBAS}\")\n",
    "print(\"\\n\")\n",
    "print(f\"* Número total de imágenes: {len(dataset)}\")\n",
    "print(f\"* Número de imágenes de entrenamiento: {len(dataset_entrenamiento)}\")\n",
    "print(f\"* Número de imágenes de validación: {len(dataset_validacion)}\")\n",
    "print(f\"* Dimensiones de las imágenes: {dataset[0][0].shape}\")\n",
    "print(\"\\n\")\n",
    "print(f\"* Número de batches de entrenamiento: {len(loader_entrenamiento)}\")\n",
    "print(f\"* Número de batches de validación: {len(loader_validacion)}\")\n",
    "print(f\"* Tamaño de cada batch de entrenamiento: {loader_entrenamiento.batch_size}\")\n",
    "print(f\"* Tamaño de cada batch de validación: {loader_validacion.batch_size}\")\n",
    "print(\"\\n\")\n",
    "print(f\"* Número de clases: {num_classes}\")\n",
    "print(f\"* Ejemplo de clases: {dataset.classes[30:34]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a construir la arquitectura de la red neuronal que aprenderá a clasificar las frutas a partir de las imágenes que le pasemos. En este caso, vamos a utilizar una red convolucional secuencial (CNN secuencial), que se caracteria por apilar capas una tras otra en orden lineal.\n",
    "\n",
    "Nuestra red va a tener varias capas convolucionales, al menos tres, para extraer características visuales (como bordes, texturas o formas), seguidas por capas de pooling que reducirán la dimensaionalidad conservando lo más importante. Al final, vamos a aplanar la salida y la pasaremos por una capa densa/completamente conectada que emitirá una probabilidad para cada clase usando softmax. Además, todas las capas ocultas llevarán activación ReLU.\n",
    "\n",
    "Como funciones de optimización, vamos a utilizar *RMSprop*, que es un optimizador adaptativo que ajusta la tasa de aprendizaje para cada parámetro. Esto nos es útil porque nos ayuda a converger más rápido y evita problemas de oscilación en la función de pérdida. La función de pérdida que vamos a utilizar es *categorical_crossentropy*, que mide la diferencia entre las probabilidades predichas por el modelo y las etiquetas reales. \n",
    "\n",
    "> **Nota:** Lo bueno de usar `CrossEntropyLoss()` es que no tenemos que preocuparnos por aplicar softmax en la última capa, ya que esta función lo hace automáticamente (internamente aplica softmax + log loss). \n",
    "\n",
    "Como hemos mencionado antes, nuestra arquitectura tiene tres bloques convolucionales: cada uno aplica filtros (pequeñas \"ventanas\" que extraen patrones visuales) y reduce la resolución con *MaxPooling* (que toma el valor máximo de cada bloque). Nuestra idea es que las primeras capas detecten características simples (como bordes), y las últimas capas detecten características más complejas (como sería la forma de la fruta).\n",
    "\n",
    "Luego usamos `Flatten()` para convertir la salida 3D a un vector 1D, que se lo pasamos a una capa densa (*Linear*). Añadimos una capa `Dropout()` para prevenir el overfitting, haciendo que nuestro modelo no se quede \"demasiado cómodo\" con los datos de entrenamiento. Finalmente, usamos una última capa *Linear* con tantas salidas coo clases tenemos.\n",
    "\n",
    "Para calcular el tamaño que tendrá la salida después de pasar por las capas de `MaxPool2d` hemos hecho el siguiente cálculo: Cada capa de *MaxPooling* con un tamaño de kernel de $2 \\times 2$ y un stride (paso) de $2$ reduce las dimensiones de la imagen a la mitad. Por lo tanto, si empezamos con una imagen de $64 \\times 64$ tendremos:\n",
    "* Después del primer *MaxPooling* (64, 64) $\\rightarrow$ (32, 32).\n",
    "* Después del segundo *MaxPooling* (32, 32) $\\rightarrow$ (16, 16).\n",
    "* Después del tercer *MaxPooling* (16, 16) $\\rightarrow$ (8, 8).\n",
    "\n",
    "Así que la salida final antes de aplanar es de $8 \\times 8$, con 128 canales (los asignamos nosotros). La salida de esta capa `Flatten()` tiene un tamaño de $8 \\times 8 \\times 128 = 8192$.\n",
    "\n",
    "> **Nota:** En la primera capa convolucional, `in_channels` es 3 porque las imágenes tienen 3 canales (RGB). En la última capa, `out_features` es el número de clases que tenemos.\n",
    "> \n",
    "> Si la imagen por ejemplo fuese en blanco y negro (escala de grises), entonces `in_channels` sería 1, porque solo habría un canal.\n",
    ">\n",
    "> Cada uno de estos canales, al final, es una matriz 2D que representa la intaensidad del color correspondiente en cada píxel. Juntos forman una imagen 3D con dimensiones $(canales, alto, ancho)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClasificadorCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        # Incializamos la clase padre nn.Module\n",
    "        # para poder usar todas sus funcionalidades\n",
    "        # (como el método forward, que define cómo seprocesa la entrada)\n",
    "        super(ClasificadorCNN, self).__init__()\n",
    "\n",
    "        self.modelo = nn.Sequential(\n",
    "            # Primera capa convolucional\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Segunda capa convolucional\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Tercera capa convolucional\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Aplanamos las dimensiones de la imagen para que\n",
    "            # se pueda usar en la capa densa (lineal).\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # Capa densa (lineal) de salida\n",
    "            nn.Linear(128 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            # Capa de salida (número de clases)\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.modelo(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias\n",
    "* [Convolutional neural network - ScienceDirect](https://www.sciencedirect.com/topics/computer-science/convolutional-neural-network)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA_II",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
