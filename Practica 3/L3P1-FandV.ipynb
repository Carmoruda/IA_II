{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 3 - CNN para clasificar imágenes de frutas - Lab 1\n",
    "## Preparación de entorno\n",
    "#### Instalar las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy\n",
    "# %pip install pandas\n",
    "# %pip install matplotlib\n",
    "# %pip install torch\n",
    "# %pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar librerías de código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN SetUp\n",
    "* **Red convolucional secuencial (CNN secuencial)** $\\rightarrow$ Tipo de red neuronal convolucional diseñada utilizando un modelo secuenncial (una pila lineal de capas donde cada capa recibe la salida de la capa anterior como entrada).\n",
    "  * **Convolución** $\\rightarrow$ Operación matemática que combina dos funciones para producir una tercera función. En el contexto de las CNN, se utiliza para extraer características de las imágenes.\n",
    "  * **Capas** $\\rightarrow$ Capas de convolución, activación, pooling y completamente conectadas (fully connected).\n",
    "  * Recibe **varias entradas** (como señales), cada una con un peso que indica su importancia.\n",
    "\n",
    "* Componentes principales de una CNN secuencial:\n",
    "  * **Capas convolucionales** $\\rightarrow$ Aplican filtros para extraer características relevantes de las imágenes, como bordes, texturas o patrones. El tamaño y número de filtros son hiperparámetros que definimos nosotros.\n",
    "  * **Funciones de activación** $\\rightarrow$ Generalmente, se utiliza *ReLU* para introducir no linealidad en las capas intermedias, mientras que *Softmax* se utiliza en la capa de salida para problemas de clasificación multiclase.\n",
    "  * **Capas de pooling** $\\rightarrow$ Reducen las dimensiones espaciales (alto y ancho) de las características manteniendo las más relevantes. Esto disminuye el coste computacional y nos ayuda a prevenir el sobreajuste.\n",
    "  * **Capas completamente conectadas** $\\rightarrow$ Al final, las características que hemos extraído las aplanamos y las pasamos a una o más capas densas (completamente conectadas) para hacer la clasificación final.\n",
    "  * **Optimizador y fución de pérdida** $\\rightarrow$ Utilizamos *RMSprop* como optimizador y *categorical_crossentropy* como función de pérdida para problemas de clasificación multiclase. Estos son hiperparámetros que podemos ajustar según nuestras necesidades.\n",
    "\n",
    "<img src=\"./media/Estructura cnn.jpg\" width=\"70%\" style=\"display: block; margin: 0 auto;\"/>\n",
    "\n",
    "<img src=\"./media/Arquitectura cnn.jpg\" width=\"70%\" style=\"display: block; margin: 0 auto; padding-top: 15px;\"/>\n",
    "\n",
    "* Aplicaciones de las redes convolucionales:\n",
    "  * **Clasificación de imágenes** $\\rightarrow$ Identificar objetos en imágenes, como en nuestro caso (clasificar frutas).\n",
    "  * **Detección de objetos** $\\rightarrow$ Localizar y clasificar múltiples objetos en una imagen.\n",
    "  * **Segmentación de imágenes** $\\rightarrow$ Dividir una imagen en regiones significativas, como identificar diferentes partes de una imagen médica.\n",
    "  * **Reconocimiento facial** $\\rightarrow$ Identificar y verificar caras en imágenes o videos.\n",
    "  * **Procesamiento de video** $\\rightarrow$ Analizar secuencias temporales como en coches autónomos o vigilancia.\n",
    "  * **Generación de imágenes** $\\rightarrow$ Crear imágenes nuevas a partir de datos existentes, como en el caso de *GANs* (Generative Adversarial Networks).\n",
    "  * **Reconocimietnto de texto** $\\rightarrow$ Extraer texto de imágenes (OCR).\n",
    "  \n",
    "### Implementación\n",
    "\n",
    "Lo primero que vamos a hacer antes de entrenar la red, es preparar adecuadamente los datos. En este caso, vamos a utilizar el dataset de frutas [*\"Fruits 360\"*](https://www.kaggle.com/datasets/moltean/fruits/data), donde las imágenes están organizadas en carpetas según su clase (es decir, cada carpeta corresponde a una fruta distinta). Esto nos va a ayudar mucho, porque podemos utlizar la función *ImageFolder* de PyTorch, que automáticamente asigna una etiqueta a cada imagen según el nombre de la carpeta en la que se encuentra.\n",
    "\n",
    "Como las redes neuronales no pueden procesar directamente JPEGs, necesitamos convertirlas a tensores, que son estructuras numéricas similares a matrices. Para ello vamos a usar una serie de transformaciones que aplicarmeros a cada imagen: primero las redimensionamos a un tamaño uniforme (64x64 píxeles), y luego las convertimos a tensores, lo cual también normaliza sus valores de píxel entre 0 y 1. Esto va ayudar a que nuestro modelo entrene de forma más estable.\n",
    "\n",
    "> **Nota:** Cuanto más grande sea la imagen, más tiempo tardará en entrenar el modelo. En este caso, hemos elegido 64x64 píxeles como un tamaño intermedio que debería funcionar bien para nuestro problema. En general, es recomendable usar imágenes de tamaño uniforme para evitar problemas de memoria y mejorar la eficiencia del entrenamiento. Sin embargo, si tenemos imágenes de diferentes tamaños, podemos usar técnicas de *data augmentation* para aumentar la diversidad del dataset y mejorar la capacidad de generalización del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas de los directorios de datos\n",
    "DIRECTORIO_ENTRENAMIENTO = './data/FandV/Training'\n",
    "DIRECTORIO_PRUEBAS = './data/FandV/Test'\n",
    "\n",
    "# Transformaciones que vamos a aplicar a las imágenes\n",
    "transformacion = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),       # Redimensionamos todas las imágenes al mismo tamaño (64x64)\n",
    "    transforms.ToTensor(),             # Convertimos la imagen a tensor (valores entre 0 y 1)\n",
    "])\n",
    "\n",
    "# Cargamos las imágenes usando ImageFolder, que usa la carpeta como etiqueta\n",
    "dataset = datasets.ImageFolder(DIRECTORIO_ENTRENAMIENTO, transform=transformacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que ya hemos cagado todas las imágenes y las tenemos en el tensor, tenemos que dividir el conjunto de datos de `DIRECTORIO_ENTRENAMIENTO` en dos subconjuntos:\n",
    "* **Conjunto de entrenamiento** $\\rightarrow$ Lo vamos a utilizar para entrenar el modelo. Este conjunto tiene la mayoría de las imágenes y es donde el modelo aprende a reconocer patrones y características de las frutas.\n",
    "* **Conjunto de validación** $\\rightarrow$ Lo utilizamos para evaluar el rendimiento del modelo durante el entrenamiento. Este conjunto tiene menos imágenes y se utiliza para comprobar si el modelo está aprendiendo correctamente y no se está sobreajustando a los datos de entrenamiento.\n",
    "\n",
    "Esta división nos va a permitir entrenar el modelo con una parte de los latos y luego comprobar como de bien generaliza con datos que no ha visto antes. Normalmente, se suele reservar un 20% de los datos para validación. Para dividir el dataset, vamos a utilizar la función `random_split` de PyTorch, que nos permite dividir un dataset en dos subconjuntos de forma aleatoria. Esta función toma como entrada el dataset original y las longitudes de los subconjuntos que queremos crear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "                             Información del dataset                             \n",
      "---------------------------------------------------------------------------------\n",
      "* Directorio de entrenamiento: ./data/FandV/Training\n",
      "* Directorio de pruebas: ./data/FandV/Test\n",
      "\n",
      "\n",
      "* Número total de imágenes: 92545\n",
      "* Número de imágenes de entrenamiento: 74036\n",
      "* Número de imágenes de validación: 18509\n",
      "* Dimensiones de las imágenes: torch.Size([3, 64, 64])\n",
      "\n",
      "\n",
      "* Número de batches de entrenamiento: 2314\n",
      "* Número de batches de validación: 579\n",
      "* Tamaño de cada batch de entrenamiento: 32\n",
      "* Tamaño de cada batch de validación: 32\n",
      "\n",
      "\n",
      "* Número de clases: 180\n",
      "* Ejemplo de clases: ['Apricot 1', 'Avocado 1', 'Avocado ripe 1', 'Banana 1']\n"
     ]
    }
   ],
   "source": [
    "# Dividimos el dataset en entrenamiento y validacion (80%-20%)\n",
    "tamano_entrenamiento = int(0.8 * len(dataset))\n",
    "tamano_validacion = len(dataset) - tamano_entrenamiento\n",
    "\n",
    "dataset_entrenamiento, dataset_validacion = random_split(dataset, [tamano_entrenamiento, tamano_validacion])\n",
    "\n",
    "# Usamos DataLoader para cargar los datos en batches\n",
    "# (batch_size=32 significa que cargamos 32 imágenes a la vez)\n",
    "# (shuffle=True significa que mezclamos los datos en cada época)\n",
    "loader_entrenamiento = DataLoader(dataset_entrenamiento, batch_size=32, shuffle=True)\n",
    "loader_validacion = DataLoader(dataset_validacion, batch_size=32, shuffle=False)\n",
    "\n",
    "# Número de clases (etiquetas diferentes)\n",
    "num_classes = len(dataset.classes)\n",
    "\n",
    "# Mostramos la información del dataset\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "print(\"                             Información del dataset                             \")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "print(f\"* Directorio de entrenamiento: {DIRECTORIO_ENTRENAMIENTO}\")\n",
    "print(f\"* Directorio de pruebas: {DIRECTORIO_PRUEBAS}\")\n",
    "print(\"\\n\")\n",
    "print(f\"* Número total de imágenes: {len(dataset)}\")\n",
    "print(f\"* Número de imágenes de entrenamiento: {len(dataset_entrenamiento)}\")\n",
    "print(f\"* Número de imágenes de validación: {len(dataset_validacion)}\")\n",
    "print(f\"* Dimensiones de las imágenes: {dataset[0][0].shape}\")\n",
    "print(\"\\n\")\n",
    "print(f\"* Número de batches de entrenamiento: {len(loader_entrenamiento)}\")\n",
    "print(f\"* Número de batches de validación: {len(loader_validacion)}\")\n",
    "print(f\"* Tamaño de cada batch de entrenamiento: {loader_entrenamiento.batch_size}\")\n",
    "print(f\"* Tamaño de cada batch de validación: {loader_validacion.batch_size}\")\n",
    "print(\"\\n\")\n",
    "print(f\"* Número de clases: {num_classes}\")\n",
    "print(f\"* Ejemplo de clases: {dataset.classes[30:34]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias\n",
    "* [Convolutional neural network - ScienceDirect](https://www.sciencedirect.com/topics/computer-science/convolutional-neural-network)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA_II",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
